# Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning

#### [Link](https://arxiv.org/abs/2205.05638)

#### Information

- Author/Institution : [Haokun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+H), [Derek Tam](https://arxiv.org/search/cs?searchtype=author&query=Tam,+D), [Mohammed Muqeeth](https://arxiv.org/search/cs?searchtype=author&query=Muqeeth,+M), [Jay Mohta](https://arxiv.org/search/cs?searchtype=author&query=Mohta,+J), [Tenghao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+T), [Mohit Bansal](https://arxiv.org/search/cs?searchtype=author&query=Bansal,+M), [Colin Raffel](https://arxiv.org/search/cs?searchtype=author&query=Raffel,+C)
- Conference/Journal : 36th Conference on Neural Information Processing Systems (NeurIPS 2022)
- Cited by 142 _ (2023.09.20)_
- Submitted on 11 May 2022 ([v1](https://arxiv.org/abs/2205.05638v1)), last revised 26 Aug 2022 (this version, v2)


### Abstract

**Few-shot in-context learning (ICL)**

- prompted exampleì„ ë„£ì–´ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ, ì ì€ ì–‘ì˜ input sample(input-target pair)ì„ ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” instruction-exampleë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì— ë„£ì–´ gradient ê¸°ë°˜ í›ˆë ¨ ì—†ì´ unseen taskë¥¼ ìˆ˜í–‰
- ëª¨ë“  prompted input-target pairë¥¼ ì²˜ë¦¬í•  ë•Œ, ì˜ˆì¸¡ì´ ìˆ˜í–‰ë  ë•Œë§ˆë‹¤ ëª¨ë“  training sampleì„ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— ìƒë‹¹í•œ computation cost ë°œìƒ
- fine-tuningë³´ë‹¤ ì„±ëŠ¥ì ìœ¼ë¡œ ì—´ë“±

**Parameter-efficient fine-tuning (PEFT)**

- ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ì ì€ ì–‘ì˜ ì¶”ê°€ëœ/ì„ íƒëœ parameterë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ fine-tuning í•¨ìœ¼ë¡œì¨ ìƒˆë¡œìš´ taskë¥¼ ì˜ ìˆ˜í–‰í•˜ë„ë¡ í•˜ëŠ” method

**ë³¸ ë…¼ë¬¸ì—ì„œì˜ PEFT**

- ICLë³´ë‹¤ PEFTê°€ ë” ì¢‹ì€ ì„±ëŠ¥/ë” ë‚®ì€ ë¹„ìš©
- í•™ìŠµëœ ë²¡í„°ë¡œ intermediate activationì„ scaling(multiply)í•˜ë©´ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ paramter ì–‘ + ë” ì¢‹ì€ ì„±ëŠ¥ ìˆ˜ë°˜í•˜ëŠ” ìƒˆë¡œìš´ PEFT method `IA` ì†Œê°œ
- prompted datasetì˜ multi-task fine-tuningì„ ìˆ˜í–‰í•˜ëŠ” T5ì˜ ë³€í˜•ì¸ `T0 ëª¨ë¸` ì‚¬ìš©
    
    â‡’ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ  task-specific tuningì´ë‚˜ modification ì—†ì´ new unseen taskì— ì ìš© ê°€ëŠ¥í•œ `T-few`ë¼ê³  ë¶ˆë¦¬ëŠ”  ìƒˆë¡­ê³  ë‹¨ìˆœí•œ ë ˆì‹œí”¼(model, PEFT method ë° fixed hyper-parameter set)ì„ ì œì•ˆ
    

### Background

**Few-shot in-context learning (ICL)**

- íŠ¹ì§•
    - ëª¨ë¸ì— â€œshotâ€ì´ë¼ê³  ë¶ˆë¦¬ëŠ” input-target exampleì„ ë„£ì–´ taskë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ìœ ë„
    - Ex) ê¸€ìë¥¼ ë‹¤ì‹œ í’€ì–´ì„œ ë‹¨ì–´ë¥¼ ì™„ì„±í•˜ëŠ” task
        - asinoc = casino, yfrogg = froggy, plesim = simple, iggestb = biggest, astedro = â€œroastedâ€
        - autoregressive language modelì— contextë¥¼ ë„£ì–´ ìƒ˜í”Œë§
    - classification taskì˜ ê²½ìš° ë¬¸ìì—´ê³¼ ê´€ë ¨ëœ ê° labelì— ëŒ€í•´ ëª¨ë¸ì´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ í• ë‹¹í•œ label ë¬¸ìì—´ì„ ì„ íƒí•˜ì—¬ labelì´ í• ë‹¹ (binary, multi classification ëª¨ë‘ì— í•´ë‹¹)
- ì¥ì 
    - fine-tuning ì—†ì´ ì¦‰ì‹œ ë‹¤ì–‘í•œ task ìˆ˜í–‰ ê°€ëŠ¥
        - ë‹¤ì–‘í•œ taskë¥¼ í˜¼í•©í•œ mixed task batchë¥¼ ì‚¬ìš©í•´ ì„œë¡œ ë‹¤ë¥¸ exampleì´ inputì—ì„œ ì„œë¡œ ë‹¤ë¥¸ contextë¥¼ ì‚¬ìš©í•´ ê°ê°ì˜ taskì— ì‚¬ìš© ê°€ëŠ¥
    - ë³´í†µ ì œí•œëœ ìˆ˜ì˜ label exampleì—ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì ì€ ë°ì´í„°ë¡œë„ íš¨ìœ¨ì  ì‘ë™
- ë‹¨ì 
    - ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ ëª¨ë“  in-context label exampleì„ ì²˜ë¦¬ â†’ `ê³„ì‚° ë¹„ìš© â†‘`
    - `ë©”ëª¨ë¦¬ ë¹„ìš©`ë„ k(k-shot ì¼ ë•Œ)ì— ë¹„ë¡€í•˜ì—¬ ì„ í˜• ì¦ê°€ + inferenceì—ì„œë„ model parameter ì €ì¥í•˜ëŠ” ë°ì— ì‚¬ìš©
    - íŠ¹ì • taskì— ëŒ€í•œ in-context exampleì„ ì €ì¥í•˜ëŠ” ë°ì— í•„ì—¬í•œ `ë””ìŠ¤í¬ ìŠ¤í† ë¦¬ì§€` í•„ìš”
    - in-context exampleì˜ ì—ì œ ìˆœì„œê°€ ëª¨ë¸ ì˜ˆì¸¡ì— í° ì˜í–¥ì„ ë¯¸ì¹¨

**Parameter-efficient fine-tuning**

- methods
    - ì´ˆê¸° methodë¡œ fixed pretrained modelì˜ layer ì‚¬ì´ì— Adapterë¥¼ ë„£ì–´ í•™ìŠµí•˜ëŠ” ë°©ë²• ì œì•ˆ
    - í›ˆë ¨í•  parameterì˜ sparse subsetì„ ì„ íƒí•˜ëŠ” method
    - Low-rank updateë¥¼ ìƒì„±í•˜ëŠ” method
    - lower-dimensional supspaceì—ì„œ optimizationì„ ìˆ˜í–‰í•˜ëŠ” method
    - hypercomplex multiplicationì„ ì‚¬ìš©í•˜ì—¬ low-rank adapterë¥¼ ì¶”ê°€í•˜ëŠ” method
    - ëª¨ë¸ì˜ input ë˜ëŠ” activationì— í•™ìŠµëœ ì—°ì† embeddingì„ ì—°ê²°í•˜ì—¬ taskë¥¼ ìˆ˜í–‰í•˜ë„ë¡ í•˜ëŠ” prompt tuning / prefix tuning
    - SOTA : ëª¨ë¸ì˜ parameter ì¤‘ ì¼ë¶€ë¶„ (0.01%)ë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ì„±ëŠ¥ì„ ë§ì¶œ ìˆ˜ ìˆìŒ
- ì¥ì 
    - training ë° ëª¨ë¸ ì €ì¥ì— ëŒ€í•œ ë©”ëª¨ë¦¬ ë¹„ìš© ë° ì €ì¥ê³µê°„ ë¹„ìš© â†“
    - íŠ¹ì • PEFT methodëŠ” mixed task batchë¥¼ ê°„ë‹¨í•˜ê²Œ í—ˆìš©í•¨
        - Ex) prompt tuning : batchì˜ ê° exampleì— ë‹¤ë¥¸ prompt embeddingì„ ì—°ê²°í•˜ì—¬ ì—¬ëŸ¬ task ìˆ˜í–‰ê°€ëŠ¥í•˜ë„ë¡ í•¨
- ë‹¨ì 
    - ëª¨ë¸ì„ ë‹¤ì‹œ parameterizeí•˜ëŠ” PEFT methodëŠ” mixed task batchì— ëŒ€í•´ ë¹„ìš©ì´ë‚˜ ë²ˆê±°ë¡œì›€ ë°œìƒ
    - ëª¨ë¸ì— layerë¥¼ ì¶”ê°€í•  ê²½ìš° ì‘ì§€ë§Œ ë¬´ì‹œí•  ìˆ˜ ì—†ëŠ” ì–‘ì˜ ê³„ì‚° ë° ë©”ëª¨ë¦¬ ë¹„ìš© ë°œìƒ
    - Fine-tuning ìì²´ì— ëŒ€í•œ ë¹„ìš© ë°œìƒ

### Designing the T-Few Recipe

ë ˆì‹œí”¼ë€ íŠ¹ì • ëª¨ë¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ì˜ë¯¸

**Model and Datasets**

<aside>
ğŸ’¡ T5 model : Text-to-Text Transfer Transformer 
- ëŒ€ëŸ‰ì˜ unlabeled text dataì— ëŒ€í•´ ê°€ë¦¬ì§€ ì•Šê³  ì–¸ì–´ ëª¨ë¸ë§ì„ í•˜ê¸° ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ encoder-decoder Transformer ëª¨ë¸

</aside>

Model

- T5ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” T0ë¥¼ ì‚¬ìš©í•˜ë©°, T0ëŠ” ì—¬ëŸ¬ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ training data ì„ê¸°ë¥¼ í†µí•´ T5ë¥¼ fine-tuningí•˜ì—¬ ë§Œë“¤ì–´ì§
- T0 ë²„ì „ìœ¼ë¡œ T0-3Bì™€ T0ê°€ ìˆëŠ”ë° ê°ê° íŒŒë¼ë¯¸í„° 30ì–µê°œ, 110ì–µê°œ ë²„ì „ â‡’ ë…¼ë¬¸ì—ì„œëŠ” T0-3B

Dataset

- ì¼ë°˜í™” ì„±ëŠ¥ ì…ì¦í•˜ê¸° ìœ„í•´ RAFT benchmark ì‚¬ìš©
- validation datasetì´ ì—†ëŠ” â€œreal worldâ€ few-shot taskì˜ ëª¨ìŒ

Comparison

- ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ìš©ì´í•˜ê²Œ ë¹„êµí•˜ê¸° ìœ„í•´ ë™ì¼í•œ ìˆ˜ì˜ few-shot learning example ì‚¬ìš©
- robustí•œ ë¹„êµë¥¼ ìœ„í•´ five few-shot datasetì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ seedë¡œ subsetì„ ìƒ˜í”Œë§í•˜ë©° ì¤‘ê°„ê°’ê³¼ ì‚¬ë¶„ìœ„ ë²”ìœ„ë¥¼ ê¸°ë¡í•¨

Evaluation

- í‰ê°€ë¥¼ í•  ë• ëª¨ë“  ê°€ëŠ¥í•œ label stringì— ëŒ€í•œ ëª¨ë¸ì˜ log-probabilityì— ìˆœìœ„ë¥¼ ë§¤ê¸°ê³  ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ì„ íƒì´ ì˜¬ë°”ë¥¸ ë‹µìœ¼ë¡œ ê°„ì£¼ë˜ëŠ”  â€œrank classificationâ€ì„ ì‚¬ìš©í•œë‹¤.

**Unlikelihood Training and Length Normalization**

 ì–¸ì–´ ëª¨ë¸ì˜ few-shot fine-tuning ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ **ë‘ ê°œì˜ ì¶”ê°€ loss term**ì„ íƒìƒ‰í•œë‹¤. ì–¸ì–´ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ input sequence $\bold x$ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ target sequenceì¸ $**y** = (y_1, y_2, â€¦, y_T)$ì˜ í™•ë¥ ì„ ë†’ì´ë„ë¡ í•˜ëŠ” cross-entropy loss $L_{LM} = - \frac{1}{T} \sum_t \text{log } p(y_t | **x**, y_{<t})$ë¡œ í›ˆë ¨ëœë‹¤. 

**unlikelihood loss**

- í‰ê°€ì— ì‚¬ìš©í•˜ëŠ” rank classificationì˜ íŠ¹ì„±ìƒ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì„ íƒì„ í•  í™•ë¥  ë¿ë§Œ ì•„ë‹ˆë¼ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì„ íƒì„ í•  í™•ë¥ ì— ëŒ€í•´ì„œë„ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— unlikelihood lossë¥¼ ì¶”ê°€
- ëª¨ë¸ì´ incorrect target sequenceë¡œë¶€í„° í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ì–µì œí•˜ëŠ” ë°ì— ì‚¬ìš©ëœë‹¤.

![UL_Loss](./image/UL_loss.png)

**Length Normalization Loss**

- ê° ì„ íƒì§€ë¥¼ í™•ë¥ ì— ë”°ë¼ ìˆœìœ„ë¥¼ ë§¤ê¸°ë©´ ëª¨ë¸ì€ ë‹µì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹µì„ â€œì„ í˜¸â€í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ rank classificationì„ ìˆ˜í–‰í•  ë•Œ length normalizationì„ ì‚¬ìš©í•´, ëª¨ë¸ì˜ ê°€ëŠ¥í•œ ë‹µë³€ ì„ íƒì— ëŒ€í•œ ì ìˆ˜ë¥¼ ê°ê° í•´ë‹¹ ì„ íƒì˜ token ìˆ˜ë¡œ ë‚˜ëˆˆë‹¤.
- ì£¼ì–´ì§„ output sequenceì˜ length-normalizaed log probabilityë¥¼ ê²Œì‚°í•˜ê³ , softmax corss-entropy lossë¥¼ ìµœì†Œí™”í•˜ì—¬ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ì„ íƒí•˜ëŠ” length-normalizad log probabilityë¥¼ ìµœëŒ€í™”

![LN_Loss](./image/LN_loss.png)

**Parameter-efficient fine-tuning with (IA; Infused Adapter by Inhibiting and Amplifying Inner Activations)**

ëª¨ë¸ í™œì„±í™”ì— ëŒ€í•œ learned vectorì™€ì˜ element-wise product(rescaling)ì„ íƒêµ¬

- íŠ¹íˆ $l \in \mathbb R^d$ê°€ learned task-specidic vectorì´ê³  $x \in \mathbb R^{T \times d}$ê°€ ê¸¸ì´ $T$ì˜ activation sequenceì¸ $l \odot x$ í˜•íƒœì˜ adaptationì„ ê³ ë ¤
- ì´ˆê¸° ì‹¤í—˜ì—ì„œ ì—°êµ¬ì§„ì€ Tansformer ëª¨ë¸ì˜ ê° activationì— ëŒ€í•œ learned rescaling vectorë¥¼ ë„ì…í•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ë°œê²¬
    - ëŒ€ì‹  self-attention ë° encoder-decoder attention mechanismì—ì„œ Key ë° Valueì— ëŒ€í•œ rescaling vector ë° position-wise feed-forward networkì˜ intermediate activationì— ëŒ€í•œ rescaling vectorë¥¼ ë„ì…í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ì¶©ë¶„í•¨ì„ ë°œê²¬
        
        ![softmax](./image/softmax.png)
        
- ê° transformer layer blockì— ë³„ë„ì˜ $l_k, l_v, l_{ff}$ vector setì„ ë„ì…
    - encoder, decoderì— ê°ê° $L(d_k + d_v + d_{ff}), L(2d_k + 2d_v + d_{ff})$ê°œì˜ íŒŒë¼ë¯¸í„° ì¶”ê°€
    - ì´ˆê¸°í™” ë‹¨ê³„ì—ì„œ ì„¸ ë²¡í„° ëª¨ë‘ 1ë¡œ ì´ˆê¸°í™”ë˜ì–´ ëª¨ë¸ì´ ê³„ì‚°í•˜ëŠ” ì „ì²´ í•¨ìˆ˜ëŠ” ë³€í•˜ì§€ ì•ŠìŒ
- $(IA)^3$ê°€ mixed task batchë¥¼ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ batchì˜ ê° activation sequenceë¥¼ í•´ë‹¹í•˜ëŠ” learned task vectorë¡œ ë³„ë„ ì²˜ë¦¬í•˜ê³  ì €ë ´í•˜ê²Œ ê³±í•  ìˆ˜ ìˆë‹¤

### T-Few recipe & $(IA)^3$ diagram

![framework](./image/framework.png)

### Experiment Result

![result1](./image/result1.png)

![result2](./image/result2.png)

### Conclusion

- T-Few : ê³„ì‚° ë¹„ìš© ì¤„ì´ë©´ì„œ ëª‡ê°€ì§€ taskì—ì„œ ICLë³´ë‹¤ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” parameter efficient solution
    - learned vectorë¡œ ë‚´ë¶€ activation ê°’ì„ ì¡°ì ˆí•˜ëŠ” $(IA)^3$ë¼ëŠ” ìƒˆë¡œìš´ PEFT method ì‚¬ìš©
    - ëª¨ë¸ ì „ì²´ fine-tuning í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ì ì€ ìˆ˜ì˜ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë¡œ ë” ì¢‹ì€ ì„±ëŠ¥ ì œê³µ
    - ë‘ ê°€ì§€ ì¶”ê°€ loss ë„ì…
        - unlikelihood loss
        - length normalization loss
    - í•˜ë‚˜ì˜ datasetì— ëŒ€í•´ single NVIDIA A100 GPUë¡œ 30ë¶„ë§Œì— fine-tuning ê°€ëŠ¥
